{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Name - Rev.get_R_est\n",
    "#__author__ = 'Prakhar MISRA'\n",
    "# Created 8/18/2019\n",
    "# Last edit 8/18/2010\n",
    "\n",
    "# code to answer reviwer commment regarding the inability of the model toaccount for secondary PM2.5 \n",
    "# hence following Upadhaye and dey 2018, we will also include meteorology \n",
    "\n",
    "# Function to multiply the cities with corresponding beta to find the Rest\n",
    "# also known as get beta\n",
    "\n",
    "\n",
    "# Important resource\n",
    "\n",
    "# following course by PSU teaches statistics with derived examples\n",
    "# Confidence interval -  https://newonlinecourses.science.psu.edu/stat414/node/297/\n",
    "# prediction interval - https://newonlinecourses.science.psu.edu/stat414/node/298/\n",
    "# CI for Multi linear regression https://newonlinecourses.science.psu.edu/stat501/lesson/7/7.1\n",
    "\n",
    "# for bayesian \n",
    "# More advanced , posterioir predictive distribution for Bayesian \n",
    "# https://baezortega.github.io/2018/08/06/robust_regression/\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import timedelta, date\n",
    "import matplotlib.dates as mdates\n",
    "import time\n",
    "import netCDF4\n",
    "import datetime as dt\n",
    "from netCDF4 import MFDataset\n",
    "from netCDF4 import num2date\n",
    "import datetime as dt  # Python standard library datetime  module\n",
    "from glob import glob\n",
    "\n",
    "currdir = os.getcwd()\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "gpath = r\"D:\\Research\\Codes_W\\GoogleTrends\\\\\"\n",
    "currdir = os.getcwd()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dtaa path \n",
    "reanalysis_dir = os.path.join(\"E:/\", \"OneDrive\", \"AQM_Research\", \"Data\", \"Raw\", \"NCEP_globalreanalysis\")\n",
    "reanalysis_dir = os.path.join(\"G:/\", \"AQM_Research\", \"Data\", \"Data_raw\", \"Weather\", \"NCEP_globalreanalysis\")\n",
    "\n",
    "\n",
    "# list of meteorological characteristics\n",
    "list_meteor = [\"tmax\", \"tmin\", \"rhum\", \"uwnd\", \"vwnd\", \"hpbl\"]\n",
    "\n",
    "#meteor path for monthly data\n",
    "tmaxfile = os.path.join(reanalysis_dir, \"tmax.2m.mon.mean.nc\") # + \"2011.nc\"\n",
    "tminfile = os.path.join(reanalysis_dir, \"tmin.2m.mon.mean.nc\")\n",
    "rhumfile = os.path.join(reanalysis_dir, \"rhum.mon.mean.nc\")\n",
    "uwndfile = os.path.join(reanalysis_dir, \"uwnd.10m.mon.mean.nc\")\n",
    "vwndfile = os.path.join(reanalysis_dir, \"vwnd.10m.mon.mean.nc\")\n",
    "\n",
    "# NOAA-CIRES 20th Century Reanalysis version 2c Monthly Averages\n",
    "# https://www.esrl.noaa.gov/psd/cgi-bin/db_search/DBSearch.pl?Variable=Planetary+Boundary+Layer+Height&group=0&submit=Search\n",
    "pblfile = os.path.join(reanalysis_dir, \"hpbl.mon.mean.nc\") \n",
    "\n",
    "#meteor dict\n",
    "dict_meteorpath = {\n",
    "    \"tmax\":tmaxfile,\n",
    "    \"tmin\":tminfile,\n",
    "    \"rhum\":rhumfile,\n",
    "    \"uwnd\":uwndfile,\n",
    "    \"vwnd\":vwndfile,\n",
    "    \"hpbl\":pblfile,\n",
    "}\n",
    "\n",
    "# file containg lat/lon\n",
    "df_loc = pd.read_csv(os.path.join(currdir, \"lib\", \"20citycoordforAQmodel.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DONT RUN AGAIN\n",
    "\n",
    "\n",
    "#citylist = [\"Delhi\", \"Bangkok\", \"Kolkata\",\"Tokyo\", \"Dhaka\", \"Karachi\", \"Taipei\", \"Tehran\", \"Seoul\", \"Manila\", \"HoChiMinh\", \"Yangon\"]\n",
    "citylist = df_loc.city_coord.unique()\n",
    "\n",
    "\n",
    "for city in citylist:\n",
    "    \n",
    "    # city\"s lat, lon\n",
    "    citylat  =  df_loc.loc[df_loc[\"city_coord\"]==city][\"lat\"]\n",
    "    citylon  =  df_loc.loc[df_loc[\"city_coord\"]==city][\"lon\"]\n",
    "\n",
    "\n",
    "    for meteor in list_meteor[:-1]:\n",
    "\n",
    "        meteorseries = []\n",
    "        timeseries = []\n",
    "\n",
    "\n",
    "        f = MFDataset(dict_meteorpath[meteor])\n",
    "        #print (f.variables)\n",
    "\n",
    "        meteorval = f.variables[meteor][:]\n",
    "        lats = f.variables['lat'][:]\n",
    "        lons = f.variables['lon'][:]\n",
    "        \n",
    "        # extract and convert time to datetime units\n",
    "        time = f.variables['time']\n",
    "        timeseries = num2date(time[:], time.units) \n",
    "\n",
    "        # Find the nearest latitude and longitude for the city\n",
    "        lat_idx = np.abs(lats - citylat.values).argmin()\n",
    "        lon_idx = np.abs(lons - citylon.values).argmin()\n",
    "        \n",
    "     \n",
    "\n",
    "        #get the values for the year\n",
    "        meteorseries = meteorval[:, :, lat_idx, lon_idx]\n",
    "\n",
    "        #print (meteorval.shape) # (2922, 73, 144) equivalent to 8 year data\n",
    "        #print (time.units) # hours since 1800-01-01 00:00:0.0\n",
    "\n",
    "        asd = pd.DataFrame({\"date\": timeseries, meteor: meteorseries[:,0]} )\n",
    "        asd[\"city_coord\"] = city\n",
    "        asd.set_index(\"date\", inplace= True)\n",
    "\n",
    "        asd.to_csv(os.path.join(currdir, \"Data\",  \"Reanalysis\",\"Monthly\" , city + \"_\" + meteor + \".csv\"))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DONT RUN AGAIN\n",
    "\n",
    "# another specially for PBL \n",
    "\n",
    "for city in citylist:\n",
    "    \n",
    "    # city\"s lat, lon\n",
    "    citylat  =  df_loc.loc[df_loc[\"city_coord\"]==city][\"lat\"]\n",
    "    citylon  =  df_loc.loc[df_loc[\"city_coord\"]==city][\"lon\"]\n",
    "\n",
    "\n",
    "    meteor = \"hpbl\"\n",
    "\n",
    "    meteorseries = []\n",
    "    timeseries = []\n",
    "\n",
    "\n",
    "    f = MFDataset(dict_meteorpath[meteor])\n",
    "    #print (f.variables)\n",
    "\n",
    "    meteorval = f.variables[meteor][:]\n",
    "    lats = f.variables['lat'][:]\n",
    "    lons = f.variables['lon'][:]\n",
    "\n",
    "    # extract and convert time to datetime units\n",
    "    time = f.variables['time']\n",
    "    timeseries = num2date(time[:], time.units) \n",
    "\n",
    "    # Find the nearest latitude and longitude for the city\n",
    "    lat_idx = np.abs(lats - citylat.values).argmin()\n",
    "    lon_idx = np.abs(lons - citylon.values).argmin()\n",
    "\n",
    "\n",
    "\n",
    "    #get the values for the year\n",
    "    meteorseries = meteorval[ :, lat_idx, lon_idx]\n",
    "\n",
    "    #print (meteorval.shape) # (2922, 73, 144) equivalent to 8 year data\n",
    "    #print (time.units) # hours since 1800-01-01 00:00:0.0\n",
    "\n",
    "    asd = pd.DataFrame({\"date\": timeseries, meteor: meteorseries} )\n",
    "    asd[\"city_coord\"] = city\n",
    "\n",
    "    asd.set_index(\"date\", inplace= True)\n",
    "\n",
    "    asd.to_csv(os.path.join(currdir, \"Data\",  \"Reanalysis\",\"Monthly\" , city + \"_\" + meteor + \".csv\"))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakhar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Prakhar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Prakhar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# combine all od them together\n",
    "\n",
    "\n",
    "list_df_city_met =[]\n",
    "for meteor in list_meteor:\n",
    "    \n",
    "    list_df = []\n",
    "    for file in glob(os.path.join(currdir, \"Data\",  \"Reanalysis\",\"Monthly\", '*'+ meteor + '.csv')):\n",
    "\n",
    "        df_meteor = pd.read_csv(file, header = 0)\n",
    "        df_meteor[\"date\"] = pd.to_datetime(df_meteor['date'], format=\"%Y-%m-%d\")\n",
    "        df_meteor = df_meteor[df_meteor.date.dt.year>=1980]\n",
    "        df_meteor.index = pd.MultiIndex.from_arrays(df_meteor[['city_coord', 'date']].values.T, names=['idxR1', 'idxR2'])\n",
    "        #df_meteor = df_meteor.set_index(\"date\")\n",
    "\n",
    "        list_df.append(df_meteor)\n",
    "    \n",
    "    temp_df = pd.concat(list_df)\n",
    "    #temp_df.index = pd.MultiIndex.from_arrays(temp_df[['city', 'date']].values.T, names=['idxR1', 'idxR2'])\n",
    "    list_df_city_met.append(temp_df)\n",
    "\n",
    "# merge all the dfferent meteors in axis = 1 this axis (horizontal) -->\n",
    "df_city_met = pd.concat(list_df_city_met, axis =1)\n",
    "\n",
    "# remove duplicate columns\n",
    "df = df_city_met\n",
    "df = df.loc[:,~df.columns.duplicated()]\n",
    "\n",
    "#correct the temp by Kelvin\n",
    "df[\"tmax\"] = df[\"tmax\"] - 273\n",
    "df[\"tmin\"] = df[\"tmin\"] - 273\n",
    "df[\"wnd\"] = (df[\"uwnd\"]**2 + df[\"vwnd\"]**2)**0.5\n",
    "\n",
    "# since PBL values are missing after 2014, replace successive years by vaues of 2014\n",
    "df.loc[df.date.dt.year== 2015, [\"hpbl\"]] = df[df.date.dt.year== 2014][\"hpbl\"].values\n",
    "df.loc[df.date.dt.year== 2016, [\"hpbl\"]] = df[df.date.dt.year== 2014][\"hpbl\"].values\n",
    "df.loc[df.date.dt.year== 2017, [\"hpbl\"]] = df[df.date.dt.year== 2014][\"hpbl\"].values\n",
    "# and save\n",
    "df.to_csv(os.path.join(currdir, \"Data\",  \"Reanalysis\",\"Monthly\", '0_allcity_meteor.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the meteordata with the file to be used for the bayesian simulation \n",
    "\n",
    "\n",
    "# open the city meteor data file\n",
    "df_city_meteor = pd.read_csv(os.path.join(currdir, \"Data\",  \"Reanalysis\",\"Monthly\", '0_allcity_meteor.csv'), header = 0)\n",
    "df_city_meteor[\"date\"] = pd.to_datetime(df_city_meteor['date'], format=\"%Y-%m-%d\")\n",
    "# set the multi index\n",
    "df_city_meteor.index = pd.MultiIndex.from_arrays(df_city_meteor[['city_coord', 'date']].values.T, names=['idx1', 'idx2'])\n",
    "\n",
    "\n",
    "# open the city data for Bayesian \n",
    "# df_allAYsetclean = pd.read_csv(os.path.join(currdir, \"my_bayesian\",  \"HBM_city_tier_20180116\",\"report20180212_IDW2_20190814\", 'allAY20180212IDW2setclean20012011.csv'))\n",
    "df_allAYsetclean = pd.read_csv(os.path.join(currdir, \"my_bayesian\",  \"HBM_city_tier_20180116\",\"report20180212_IDW2_20190814\", \"meteorology_model\", 'allAY20180212IDW2setclean.csv'))\n",
    "df_allAYsetclean[\"date\"] = pd.to_datetime(df_allAYsetclean['date'], format=\"%m/%d/%Y\")\n",
    "# set the mutu index\n",
    "df_allAYsetclean.index = pd.MultiIndex.from_arrays(df_allAYsetclean[['citychk_loc', 'date']].values.T, names=['idxR1', 'idxR2'])\n",
    "df_allAYsetclean.drop(columns = [\"citychk_loc\",  \"date\"], inplace = True)\n",
    "\n",
    "# and merge on the basis of multiindex\n",
    "\n",
    "# need to give name to index level\n",
    "df_allAYsetclean.index.levels[0].name = \"idx1\"\n",
    "df_allAYsetclean.index.levels[1].name = \"idx2\"\n",
    "df_city_meteor.index.levels[0].name = \"idx1\"\n",
    "df_city_meteor.index.levels[1].name = \"idx2\"\n",
    "\n",
    "\n",
    "# now merge\n",
    "df_merge = pd.merge(df_allAYsetclean, df_city_meteor, how='inner', on=None, left_on=None, right_on=None,\n",
    "         left_index=True, right_index=True, sort=True,\n",
    "         suffixes=('_x', '_y'), copy=True, indicator=False,\n",
    "         validate=None)\n",
    "\n",
    "# and save\n",
    "# df_merge.to_csv(os.path.join(currdir, \"my_bayesian\",  \"HBM_city_tier_20180116\",\"report20180212_IDW2_20190814\", 'allAY20180212IDW2setclean20012011_meteor.csv'))\n",
    "df_merge.to_csv(os.path.join(currdir, \"my_bayesian\",  \"HBM_city_tier_20180116\",\"report20180212_IDW2_20190814\", \"meteorology_model\", 'allAY20180212IDW2setclean_meteor.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
